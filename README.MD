# ğŸ§  NextWordD: Context-Based Word Generator with MLP

**NextWordMLP** is a simple neural language model that predicts the next word in a sentence using only a **Multi-Layer Perceptron (MLP)**. It is trained on the classic text *The Adventures of Sherlock Holmes* and supports user-adjustable parameters like **block size** and **embedding dimension**.

This project demonstrates that even without RNNs or Transformers, meaningful language modeling is possible with careful design and sufficient context.

---

## ğŸ¯ Try it Live

ğŸ‘‰ **[Streamlit App](https://nextwordd.streamlit.app/)**

The app lets you:
- Select a **block size** (context length)
- Tune the **embedding dimension**
- Generate words from a live MLP model
- Observe how the model behaves with different hyperparameters

---

## âš™ï¸ Features

- ğŸ”¸ Trainable word **embeddings** (`nn.Embedding`)
- ğŸ”¸ Fully connected **MLP** with ReLU or tanh activation function
- ğŸ”¸ Tokenization and vocabulary built from Sherlock Holmes corpus
- ğŸ”¸ Streamlit interface for real-time interaction

---

## ğŸ§  Methodology

### 1. Preprocessing
- Text cleaned and tokenized into words
- Vocabulary indexed as `{word: id}` and `{id: word}`
- Inputs: sliding window of `block_size` words â†’ target: next word

### 2. Model Architecture
```text
[ context (N word indices) ]
           â†“
   Embedding Layer
           â†“
   Flattened Embeddings
           â†“
   Linear â†’ ReLU â†’ Dropout â†’ Linear
           â†“
 [ logits over vocabulary ]

### 3. PyTorch Skeleton:

```python
class NextWord(nn.Module):
    def __init__(self, block_size, vocab_size, emb_dim, hidden_size=1024):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim)
        self.lin1 = nn.Linear(block_size * emb_dim, hidden_size)
        self.lin2 = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        x = self.emb(x)
        x = x.view(x.shape[0], -1)
        x = torch.tanh(self.lin1(x))  # Use tanh or relu activation
        x = self.lin2(x)
        return x

## ğŸ“ Repository Structure

NextWordMLP/<br>
â”œâ”€â”€ text_model_state.pth # model saved using torch.save<br>
â”œâ”€â”€ notebook/ # Jupyter notebooks for training & analysis<br>
â”œâ”€â”€ requirements.txt # Python dependencies<br>
â””â”€â”€ README.md # This file

## ğŸ›  Tools & Libraries

- PyTorch â€“ Model training & embedding
- Streamlit â€“ Interactive web interface
- Matplotlib â€“ Visualizations

## ğŸ‘¥ Contributors

- **A.V.S Manoj** (23110025) â€“ [manoj.anaparthi@iitgn.ac.in](mailto:manoj.anaparthi@iitgn.ac.in)  
- **N. Eshwar** (23110215) â€“ [eshwar.nakka@iitgn.ac.in](mailto:eshwar.nakka@iitgn.ac.in)  
- **O. Akash** (23110225) â€“ [23110225@iitgn.ac.in](mailto:23110225@iitgn.ac.in)
- **P. Praneeth** (23110226) â€“ [pabbathi.praneeth@iitgn.ac.in](mailto:pabbathi.praneeth@iitgn.ac.in)

---
